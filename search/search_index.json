{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my public notes # The sole purpose of this site is to share byte-sized knowledge with the world on various topics - from software engineering, to productivity, and all in between. It also provides a prettier UI and a decent search, as opposed to other alternatives. I hope you find it useful. Blog Posts vs This # Blogs require a lot of effort to properly write, curated and tell a story. Quick notes, on the other hand, are byte-sized information which I write when I encounter something new. This practice also helps me retaining knowledge. Often, you'll only see notes which I couldn't find in other places, or these are usually summaries of many sources I searched into and so on. NOTICE: This content is released under CC BY-NC-ND 3.0 license, meaning that it may be copied but must remain in identical form and that no content can be created that contains it other than fair-use citations with attribution. Copying this content for any commercial purpose is also not allowed. Licenses asides, I am not liable nor responsible for any misuse of the content or knowledge hereby written. If you like this site and would like to sponsor it, you can buying me a coffee . All the support is greatly appreciated.","title":"Home"},{"location":"#welcome-to-my-public-notes","text":"The sole purpose of this site is to share byte-sized knowledge with the world on various topics - from software engineering, to productivity, and all in between. It also provides a prettier UI and a decent search, as opposed to other alternatives. I hope you find it useful.","title":"Welcome to my public notes"},{"location":"#blog-posts-vs-this","text":"Blogs require a lot of effort to properly write, curated and tell a story. Quick notes, on the other hand, are byte-sized information which I write when I encounter something new. This practice also helps me retaining knowledge. Often, you'll only see notes which I couldn't find in other places, or these are usually summaries of many sources I searched into and so on. NOTICE: This content is released under CC BY-NC-ND 3.0 license, meaning that it may be copied but must remain in identical form and that no content can be created that contains it other than fair-use citations with attribution. Copying this content for any commercial purpose is also not allowed. Licenses asides, I am not liable nor responsible for any misuse of the content or knowledge hereby written. If you like this site and would like to sponsor it, you can buying me a coffee . All the support is greatly appreciated.","title":"Blog Posts vs This"},{"location":"notes/","text":"2025-09-26 10:45: 13 - Lazygit and GPG 2024-12-06 15:41: 12 - Mocking full classes in Python 2024-11-20 10:20: 11 - pyinstaller lib issues 2024-06-17 00:06: 10 - WordPress Reproducible Build 2024-06-12 21:57: 9 - TIL: QuerySets Over Multiple Tables May Not Return Unique Values 2024-06-06 20:08: 8 - Use bulks, please 2024-05-15 16:19: 7 - Django and Reverse M2M Relationship Filter 2024-05-13 22:54: 6 - Change Freezes Done Right 2024-05-06 14:05: 5 - Getting Started in CyberSec in Portugal 2024-05-03 22:58: 4 - Pyinstaller for Python apps bundling 2024-05-03 22:49: 3 - Proper Python Logging 2024-05-02 16:58: 2 - Updating your host's SSH keys 2024-04-27 23:30: 1 - Hello, from mkdocs","title":"Archive"},{"location":"notes/1/","text":"Hello, from mkdocs # For some time, I have been a fan of the Zettelkasten method and markdown, and had a self-proclaimed goal of building a site where I could share byte-sized notes about topics I deal with at work - software engineering, python and golang, cyber security, and other productivity related topics. After seeing what rwxrob 's Zettelkasten has evolved into, I finally found the right setup to accomplish the above. The two main reasons I am favouring mkdocs over other static site generators like Hugo are: Pure markdown support. Built-in search engine for keywords - no more need for tags or extra work to build a search engine for my own site. I still had the overhead of building the scripts to automatically create new notes and get them indexed for the main page but that was also a fun challenge to tackle.","title":"Hello, from mkdocs"},{"location":"notes/1/#hello-from-mkdocs","text":"For some time, I have been a fan of the Zettelkasten method and markdown, and had a self-proclaimed goal of building a site where I could share byte-sized notes about topics I deal with at work - software engineering, python and golang, cyber security, and other productivity related topics. After seeing what rwxrob 's Zettelkasten has evolved into, I finally found the right setup to accomplish the above. The two main reasons I am favouring mkdocs over other static site generators like Hugo are: Pure markdown support. Built-in search engine for keywords - no more need for tags or extra work to build a search engine for my own site. I still had the overhead of building the scripts to automatically create new notes and get them indexed for the main page but that was also a fun challenge to tackle.","title":"Hello, from mkdocs"},{"location":"notes/10/","text":"WordPress Reproducible Build # Had a task recently that essentially involved automating a Docker deployment of a WordPress blog. It was purely for testing purposes, so please note that most of its configuration is insecure as hell. Nonetheless, I spent 4 hours debugging an issue I thought was worth sharing and key to this task: I wanted the Docker containerization process to programmatically handle user creation and plugin installation. The idea is that we might need to frequently discard the blog, and we don't want to go through all the trouble of installing WordPress, installing specific versions of the plugins we want, and creating users with different privilege levels each time. Ultimately, we don't really care for most of that, so it's okay to use insecure defaults. So, we have a project tree that contains a folder ./plugins/vulnerable/ and some other dependency plugins that are required to use/activate in ./plugins/core . I spent 3 hours debugging why wp-cli was not installing the plugins due to an unforeseen need to escape the $ in the bash commands we were using just to get this going. PS: I am aware that using volumes to store the database and WordPress data would be better than bind mounts. For this case, this is sufficient really, and we don't want to over-engineer things 1 . For a serious project, I'd recommend using volumes. The attached link explains this better than I can. services: db: image: mysql:8.0 restart: always environment: ... volumes: - ./data/db:/var/lib/mysql wordpress: image: wordpress:6.5.2-php8.3-apache depends_on: - db restart: always ports: - 8080:80 volumes: - ./data/wordpress:/var/www/html - ./plugins:/tmp/plugins wp-cli: env_file: .env image: wordpress:cli-php8.3 user: \"33:33\" # hack for https://stackoverflow.com/questions/57136171/execute-a-plugin-installed-wp-cli-command-with-docker-compose depends_on: - wordpress volumes_from: - wordpress:rw command: > /bin/sh -c ' sleep 10; wp core install --path=\"/var/www/html\" --url=\"http://localhost:8080\" --title=\"DVWPS\" --admin_user=admin --admin_password=secret --admin_email=x@x.com; wp user create tester y@y.com --user_pass=megasecret --role=subscriber; for plugin in /tmp/plugins/**/*.zip; do wp plugin install $${plugin} --activate; done;' That $${plugin} is the difference between wp-cli being capable of installing the plugins or not. I couldn't see why the compose service was returning empty strings, even when I tried for plugin in /tmp/plugins/**/*.zip; do echo $plugin; done; and was too focused on directory permissions. In this case, it felt reasonable as the wp-cli needs to be capable of writing to directories that belong to another container in this setup. I tried a bunch of things until eventually, I tried installing one plugin I knew existed - it worked. \"Damn, this has to be an issue with the bash commands then,\" and it was... As for the user: \"33:33\" directive, I kept encountering permission issues because WordPress Dockerfile sets most directories for user www-data and the wp-cli runs with a different user. Furthermore, this is a shared bind volume, and it seems volumes_from isn't sufficient. This appears to be a known problem already, highlighted in that StackOverflow question. This fakes the running user UID and GID as www-data . https://docs.docker.com/storage/volumes/ \u21a9","title":"WordPress Reproducible Build"},{"location":"notes/10/#wordpress-reproducible-build","text":"Had a task recently that essentially involved automating a Docker deployment of a WordPress blog. It was purely for testing purposes, so please note that most of its configuration is insecure as hell. Nonetheless, I spent 4 hours debugging an issue I thought was worth sharing and key to this task: I wanted the Docker containerization process to programmatically handle user creation and plugin installation. The idea is that we might need to frequently discard the blog, and we don't want to go through all the trouble of installing WordPress, installing specific versions of the plugins we want, and creating users with different privilege levels each time. Ultimately, we don't really care for most of that, so it's okay to use insecure defaults. So, we have a project tree that contains a folder ./plugins/vulnerable/ and some other dependency plugins that are required to use/activate in ./plugins/core . I spent 3 hours debugging why wp-cli was not installing the plugins due to an unforeseen need to escape the $ in the bash commands we were using just to get this going. PS: I am aware that using volumes to store the database and WordPress data would be better than bind mounts. For this case, this is sufficient really, and we don't want to over-engineer things 1 . For a serious project, I'd recommend using volumes. The attached link explains this better than I can. services: db: image: mysql:8.0 restart: always environment: ... volumes: - ./data/db:/var/lib/mysql wordpress: image: wordpress:6.5.2-php8.3-apache depends_on: - db restart: always ports: - 8080:80 volumes: - ./data/wordpress:/var/www/html - ./plugins:/tmp/plugins wp-cli: env_file: .env image: wordpress:cli-php8.3 user: \"33:33\" # hack for https://stackoverflow.com/questions/57136171/execute-a-plugin-installed-wp-cli-command-with-docker-compose depends_on: - wordpress volumes_from: - wordpress:rw command: > /bin/sh -c ' sleep 10; wp core install --path=\"/var/www/html\" --url=\"http://localhost:8080\" --title=\"DVWPS\" --admin_user=admin --admin_password=secret --admin_email=x@x.com; wp user create tester y@y.com --user_pass=megasecret --role=subscriber; for plugin in /tmp/plugins/**/*.zip; do wp plugin install $${plugin} --activate; done;' That $${plugin} is the difference between wp-cli being capable of installing the plugins or not. I couldn't see why the compose service was returning empty strings, even when I tried for plugin in /tmp/plugins/**/*.zip; do echo $plugin; done; and was too focused on directory permissions. In this case, it felt reasonable as the wp-cli needs to be capable of writing to directories that belong to another container in this setup. I tried a bunch of things until eventually, I tried installing one plugin I knew existed - it worked. \"Damn, this has to be an issue with the bash commands then,\" and it was... As for the user: \"33:33\" directive, I kept encountering permission issues because WordPress Dockerfile sets most directories for user www-data and the wp-cli runs with a different user. Furthermore, this is a shared bind volume, and it seems volumes_from isn't sufficient. This appears to be a known problem already, highlighted in that StackOverflow question. This fakes the running user UID and GID as www-data . https://docs.docker.com/storage/volumes/ \u21a9","title":"WordPress Reproducible Build"},{"location":"notes/11/","text":"pyinstaller lib issues # As mentioned in previous notes 1 , I was working on a library that was bundled with pyinstaller , allegedly for easier distribution. However, later on I found out a very odd error when invoking some of the commands from within that library. Although I cannot share many details, I can say the library invokes some git commands, and we started having lots of issues with it as it just didn't work. /bin/ssh: symbol lookup error: /lib64/libk5crypto.so.3: undefined symbol: EVP_KDF_ctrl, version OPENSSL_1_1_1b The first test I performed was invoking the commands manually in the machine where this runs - and it worked. Thus, seems the culprit would be somewhere on packing and distribution of the library. After some research, I found out this type of issues commonly occur when applications expect certain libraries to exist and/or were compiled using different versions of the announced libraries. It came to mind that \"compiling\" this program in a debian-based image to run in a CentOS-based host was thus not a good idea. For that, I rewrote things to build the binary in a CentOS-based image. But it didn't solve the problem, and increased our build times in 3 hours (!!!) because apparently building Python from scratch can be quite expensive. So I tried compiling the binary directly in the target machine - and it worked. The prevailing theory is pyinstaller bundles all the libraries that it will require to work based on what's available in the machine, which includes low-level libraries: The one executable file contains an embedded archive of all the Python modules used by your script, as well as compressed copies of any non-Python support files (e.g. .so files). The bootloader uncompresses the support files and writes copies into the the temporary folder. This can take a little time. That is why a one-file app is a little slower to start than a one-folder app From their docs: https://pyinstaller.org/en/stable/operating-mode.html#bundling-to-one-file However, the commands my program invokes are OS-based, using Python's subprocess.run 2 . So, it was not bundling libk5crypto.so correctly, meaning the bundled binary was expecting libs in different places from the ones in the target machine. This BugZilla 3 issue does resonated a bit, but didn't provide a solution to the problem. https://gsilvapt.github.io/zet/4 \u21a9 https://docs.python.org/3/library/subprocess.html#subprocess.run \u21a9 https://bugzilla.redhat.com/show_bug.cgi?id=1829790 \u21a9","title":"pyinstaller lib issues"},{"location":"notes/11/#pyinstaller-lib-issues","text":"As mentioned in previous notes 1 , I was working on a library that was bundled with pyinstaller , allegedly for easier distribution. However, later on I found out a very odd error when invoking some of the commands from within that library. Although I cannot share many details, I can say the library invokes some git commands, and we started having lots of issues with it as it just didn't work. /bin/ssh: symbol lookup error: /lib64/libk5crypto.so.3: undefined symbol: EVP_KDF_ctrl, version OPENSSL_1_1_1b The first test I performed was invoking the commands manually in the machine where this runs - and it worked. Thus, seems the culprit would be somewhere on packing and distribution of the library. After some research, I found out this type of issues commonly occur when applications expect certain libraries to exist and/or were compiled using different versions of the announced libraries. It came to mind that \"compiling\" this program in a debian-based image to run in a CentOS-based host was thus not a good idea. For that, I rewrote things to build the binary in a CentOS-based image. But it didn't solve the problem, and increased our build times in 3 hours (!!!) because apparently building Python from scratch can be quite expensive. So I tried compiling the binary directly in the target machine - and it worked. The prevailing theory is pyinstaller bundles all the libraries that it will require to work based on what's available in the machine, which includes low-level libraries: The one executable file contains an embedded archive of all the Python modules used by your script, as well as compressed copies of any non-Python support files (e.g. .so files). The bootloader uncompresses the support files and writes copies into the the temporary folder. This can take a little time. That is why a one-file app is a little slower to start than a one-folder app From their docs: https://pyinstaller.org/en/stable/operating-mode.html#bundling-to-one-file However, the commands my program invokes are OS-based, using Python's subprocess.run 2 . So, it was not bundling libk5crypto.so correctly, meaning the bundled binary was expecting libs in different places from the ones in the target machine. This BugZilla 3 issue does resonated a bit, but didn't provide a solution to the problem. https://gsilvapt.github.io/zet/4 \u21a9 https://docs.python.org/3/library/subprocess.html#subprocess.run \u21a9 https://bugzilla.redhat.com/show_bug.cgi?id=1829790 \u21a9","title":"pyinstaller lib issues"},{"location":"notes/12/","text":"Mocking full classes in Python # In a project I am working on, I wanted to mock an entire class which comprehends a wrapper for API calls to a service. This class is tested thoroughly on its own package and I'm against performing network calls in unit tests, precisely and that could clutter up a real instance with garbage from automated tests. This is a classic use case for mocks 1 . You want to isolate a part of your program's behavior. Python, the language of the project, has a very nice library for mocking and stubbing 2 . It does so much, I am not going to even start covering it in this post. But let's do a quick dummy example to showcase the intention: # a.py class MyAPI: def does_something(self): \"\"\"Does something really cool that you want to abstract from your tests\"\"\" pass def does_something_else(self): \"\"\"Does something else even cooler but you still want to abstract from your tests\"\"\" pass # b.py def main(): # something happens api = MyAPI() api.does_something() # something else happens api.does_something_selse() return 1 # tests.py from unittest import TestCase from unittest.mock import patch class MyAPITests(unittest.TestCase): @patch(\"a.MyAPI\") def test_it_does_something(self, my_mock): assert b.main() == 1 In a nutshell, the contrived example just makes an object available and we want to test only the main of our b.py file. I find it a good practice, however, to still ensure the methods of the mocked classed are called. If your code's logic deviates and suddenly you are no longer performing the second call, for instance as you are returning earlier, your program is likely bugged. You still don't care about the mocked behaviour, you just want to ensure it's called each time. For that, your test code must change, because the classic patch does not include the spec 3 and this is the only way you can obtain a list of functions called through your mock - spec goes even further by ensuring the method signatures add up, for instance, which I find amazing. @patch(\"a.MyAPI\", autospec=True) def test_it_does_something(self, my_mock): assert b.main() == 1 assert len(my_mock.method_calls) == 2 In the second snippet, we are ensuring there were two method calls against our my_mock object. This ensures the business requirement of performing those actions is there, even though we are ignoring if they are done correctly. This isn't integration tests anyway, so I find this a quite decent way to write the test in this scenario. Needless to say I spent a few hours trying to understand why without autospec I wasn't able to reliably get the list of calls from the Mock :)) https://martinfowler.com/articles/mocksArentStubs.html \u21a9 https://docs.python.org/3/library/unittest.mock.html \u21a9 https://docs.python.org/3/library/unittest.mock.html#unittest.mock.patch \u21a9","title":"Mocking full classes in Python"},{"location":"notes/12/#mocking-full-classes-in-python","text":"In a project I am working on, I wanted to mock an entire class which comprehends a wrapper for API calls to a service. This class is tested thoroughly on its own package and I'm against performing network calls in unit tests, precisely and that could clutter up a real instance with garbage from automated tests. This is a classic use case for mocks 1 . You want to isolate a part of your program's behavior. Python, the language of the project, has a very nice library for mocking and stubbing 2 . It does so much, I am not going to even start covering it in this post. But let's do a quick dummy example to showcase the intention: # a.py class MyAPI: def does_something(self): \"\"\"Does something really cool that you want to abstract from your tests\"\"\" pass def does_something_else(self): \"\"\"Does something else even cooler but you still want to abstract from your tests\"\"\" pass # b.py def main(): # something happens api = MyAPI() api.does_something() # something else happens api.does_something_selse() return 1 # tests.py from unittest import TestCase from unittest.mock import patch class MyAPITests(unittest.TestCase): @patch(\"a.MyAPI\") def test_it_does_something(self, my_mock): assert b.main() == 1 In a nutshell, the contrived example just makes an object available and we want to test only the main of our b.py file. I find it a good practice, however, to still ensure the methods of the mocked classed are called. If your code's logic deviates and suddenly you are no longer performing the second call, for instance as you are returning earlier, your program is likely bugged. You still don't care about the mocked behaviour, you just want to ensure it's called each time. For that, your test code must change, because the classic patch does not include the spec 3 and this is the only way you can obtain a list of functions called through your mock - spec goes even further by ensuring the method signatures add up, for instance, which I find amazing. @patch(\"a.MyAPI\", autospec=True) def test_it_does_something(self, my_mock): assert b.main() == 1 assert len(my_mock.method_calls) == 2 In the second snippet, we are ensuring there were two method calls against our my_mock object. This ensures the business requirement of performing those actions is there, even though we are ignoring if they are done correctly. This isn't integration tests anyway, so I find this a quite decent way to write the test in this scenario. Needless to say I spent a few hours trying to understand why without autospec I wasn't able to reliably get the list of calls from the Mock :)) https://martinfowler.com/articles/mocksArentStubs.html \u21a9 https://docs.python.org/3/library/unittest.mock.html \u21a9 https://docs.python.org/3/library/unittest.mock.html#unittest.mock.patch \u21a9","title":"Mocking full classes in Python"},{"location":"notes/13/","text":"Lazygit and GPG # Ever since I found out anyone can sign commits using anyone's email, I decided to use GPG for git development work, whether it's personal or professional, whether people appreciate it or not. Some open-source projects these days require it, and I want to believe some companies have started to enforce this on their employees, with all the supply chain attacks and forms of compromise. Thus, I have a whole section in my gitconfig file dedicated to GPG signing. And it let's me do cool things too. Usually, there's a global key assigned in my $HOME/.gitconfig : [user] email = <my_mail> name = <the name I want to see in my commits> signingkey = DEADB33FD34DB33f In all git operations, this default configuration file is used. However, git allows local configurations, meaning users can set specific attributes for specific repositories. That said, I created a cute zsh alias called set_personal that I use to set a different signing key and email: alias set_personal='git config --local user.signingkey PERSONALD34DB33F && git config --local user.email <another_email>' Of course, more parameters could be configured this way but that's enough for me. This get's stored in <repo>/.git/config file and overrides the configuration from $HOME/.gitconfig . Pretty neat. But the problem came when I wanted to use Lazygit . I love the terminal and I think Lazygit is one of the best interfaces for Git. It's way prettier, cleaner and more intuitive than tig . I would like to find something similar to magit for neovim, but so far I never did. The problem with Lazygit is that it fails to manage GPG signing. I've come across so many problems 1 2 , it's not even funny. The latest time this happened it froze the gpg-agent and I could not even proceed manually, which is what I normally do. And as opposed to what some issues in the repository mention, I don't think it's only on pinentry programs - I have experienced some issues, albeit less, on Linux environments. Solution? Well, I only use lazygit now to do advanced adds - like when adding only a portion of changes as I always fail to use git add -p <file> . After that, I quit and proceed manually. After coming across this 3 post, I also felt less and less the need to use Lazygit . Especially the diff changes are astonishingly better. https://github.com/jesseduffield/lazygit/issues/4668 \u21a9 https://github.com/jesseduffield/lazygit/issues/3308 \u21a9 https://blog.gitbutler.com/how-git-core-devs-configure-git \u21a9","title":"Lazygit and GPG"},{"location":"notes/13/#lazygit-and-gpg","text":"Ever since I found out anyone can sign commits using anyone's email, I decided to use GPG for git development work, whether it's personal or professional, whether people appreciate it or not. Some open-source projects these days require it, and I want to believe some companies have started to enforce this on their employees, with all the supply chain attacks and forms of compromise. Thus, I have a whole section in my gitconfig file dedicated to GPG signing. And it let's me do cool things too. Usually, there's a global key assigned in my $HOME/.gitconfig : [user] email = <my_mail> name = <the name I want to see in my commits> signingkey = DEADB33FD34DB33f In all git operations, this default configuration file is used. However, git allows local configurations, meaning users can set specific attributes for specific repositories. That said, I created a cute zsh alias called set_personal that I use to set a different signing key and email: alias set_personal='git config --local user.signingkey PERSONALD34DB33F && git config --local user.email <another_email>' Of course, more parameters could be configured this way but that's enough for me. This get's stored in <repo>/.git/config file and overrides the configuration from $HOME/.gitconfig . Pretty neat. But the problem came when I wanted to use Lazygit . I love the terminal and I think Lazygit is one of the best interfaces for Git. It's way prettier, cleaner and more intuitive than tig . I would like to find something similar to magit for neovim, but so far I never did. The problem with Lazygit is that it fails to manage GPG signing. I've come across so many problems 1 2 , it's not even funny. The latest time this happened it froze the gpg-agent and I could not even proceed manually, which is what I normally do. And as opposed to what some issues in the repository mention, I don't think it's only on pinentry programs - I have experienced some issues, albeit less, on Linux environments. Solution? Well, I only use lazygit now to do advanced adds - like when adding only a portion of changes as I always fail to use git add -p <file> . After that, I quit and proceed manually. After coming across this 3 post, I also felt less and less the need to use Lazygit . Especially the diff changes are astonishingly better. https://github.com/jesseduffield/lazygit/issues/4668 \u21a9 https://github.com/jesseduffield/lazygit/issues/3308 \u21a9 https://blog.gitbutler.com/how-git-core-devs-configure-git \u21a9","title":"Lazygit and GPG"},{"location":"notes/2/","text":"Updating your host's SSH keys # If you have servers which you connect to frequently via SSH and those servers happen to be under frequent deploys, your SSH keys might no longer work with an error similar to: <username>@<hostname>: Permission denied (publickey,password). This happens because the resulting server public key you get after the initial exchange is no longer valid as the server was destroyed and brought back up. A simple solution is to simply run ssh-keygen -R <hostname> to remove the old key and then try to connect again.","title":"Updating your host's SSH keys"},{"location":"notes/2/#updating-your-hosts-ssh-keys","text":"If you have servers which you connect to frequently via SSH and those servers happen to be under frequent deploys, your SSH keys might no longer work with an error similar to: <username>@<hostname>: Permission denied (publickey,password). This happens because the resulting server public key you get after the initial exchange is no longer valid as the server was destroyed and brought back up. A simple solution is to simply run ssh-keygen -R <hostname> to remove the old key and then try to connect again.","title":"Updating your host's SSH keys"},{"location":"notes/3/","text":"Proper Python Logging # After 3 months of working on a project from scratch, I finally understood how to setup a proper logging configuration in Python. Took me long enough but I'm glad I finally got it right. On the other hand, it's frustrating to see how little I was missing... So, here's what I had: import logging def setup_logging(debug: bool): logger.setLevel(logging.DEBUG if debug else logging.INFO) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Oddly enough, at least for me at the time, I wasn't getting any logging.info() calls getting printed to the console. However, if I provided the --debug flag to the program, it would print stuff and with the correct level call ( debug , info , error , etc). I was puzzled by this behaviour and just had to focus on other parts of the program until recently. As I am finalising the project and I wanted to add proper interaction from the program to the user, logging.info() had to work. After many hours... Here's what I was missing: import logging import sys logger = logging.getLogger(__name__) def setup_logging(debug: bool): logging.setLevel(logging.DEBUG if debug else logging.INFO, stream=sys.stdout) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Now all calls get printed. Frustrating to say the least, but often times we encounter these weird challenges where the solution is really that simple.","title":"Proper Python Logging"},{"location":"notes/3/#proper-python-logging","text":"After 3 months of working on a project from scratch, I finally understood how to setup a proper logging configuration in Python. Took me long enough but I'm glad I finally got it right. On the other hand, it's frustrating to see how little I was missing... So, here's what I had: import logging def setup_logging(debug: bool): logger.setLevel(logging.DEBUG if debug else logging.INFO) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Oddly enough, at least for me at the time, I wasn't getting any logging.info() calls getting printed to the console. However, if I provided the --debug flag to the program, it would print stuff and with the correct level call ( debug , info , error , etc). I was puzzled by this behaviour and just had to focus on other parts of the program until recently. As I am finalising the project and I wanted to add proper interaction from the program to the user, logging.info() had to work. After many hours... Here's what I was missing: import logging import sys logger = logging.getLogger(__name__) def setup_logging(debug: bool): logging.setLevel(logging.DEBUG if debug else logging.INFO, stream=sys.stdout) def main(): args = argparser.parse_args() setup_logging(args.debug) logger.info(\"Starting the program...\") ... Now all calls get printed. Frustrating to say the least, but often times we encounter these weird challenges where the solution is really that simple.","title":"Proper Python Logging"},{"location":"notes/4/","text":"Pyinstaller for Python apps bundling # I'm working on a project that will run on plethora of systems and for a long time I knew one of the most hated things about Python is it's dependency system and the hassle of preparing a safe environment without messing with your system's installation. There are many tools to ease this off, like poetry , pipenv , virtualenv , pyenv , etc but they all require you to still control and maintain a dependency file somewhere, install in the systems you want your application to run and so on. Sometimes all we want from our programs is to simply run. In addition, sometimes our programs will run in systems we have minimal or no control of at all. Hence, being able to run a program without worrying about it's dependencies is a nice feature. This is one of the things I really love about Go 's standard tooling, where you get a binary with everything bundled in and ready to run - dependency free. After some investigations, I found out about pyinstaller , a tool that bundles a Python application and all its dependencies into a single package. At first glance, I thought this was going to work regardless of the Python installation in the system - just a single \"binary\" users could run like anything else in the $PATH and voil\u00e1. Well, it wasn't that simple, but the other requirements were still met by this library. Despite requiring a Python installation in the system which your program supports, which I found later when I was trying to run a Python 3.12 application in a system that only had Python 3.8 installed, all dependencies are bundled and shipped together. pyinstaller docs are great and will let you be self-sufficient for your use cases - no need to explain what I need as it was really simple. One note is that pyinstaller 's will build a file for the platform you're building from (so, x86_64 linux will only work in x86_64 linux, arm will work in arm, etc) but this is a limitation I can live with. On my development machine I can generate and run the ARM builds, and the project's pipelines take care of building and publishing the binaries for the systems where they will run. The main advantage in my perspective is that everything gets bundled into a single executable and you don't need to care about environments, dependencies and what-not. Perhaps this will lead to a bigger file size, but it's a trade-off I'm willing to take for the sake of simplicity during deployments.","title":"Pyinstaller for Python apps bundling"},{"location":"notes/4/#pyinstaller-for-python-apps-bundling","text":"I'm working on a project that will run on plethora of systems and for a long time I knew one of the most hated things about Python is it's dependency system and the hassle of preparing a safe environment without messing with your system's installation. There are many tools to ease this off, like poetry , pipenv , virtualenv , pyenv , etc but they all require you to still control and maintain a dependency file somewhere, install in the systems you want your application to run and so on. Sometimes all we want from our programs is to simply run. In addition, sometimes our programs will run in systems we have minimal or no control of at all. Hence, being able to run a program without worrying about it's dependencies is a nice feature. This is one of the things I really love about Go 's standard tooling, where you get a binary with everything bundled in and ready to run - dependency free. After some investigations, I found out about pyinstaller , a tool that bundles a Python application and all its dependencies into a single package. At first glance, I thought this was going to work regardless of the Python installation in the system - just a single \"binary\" users could run like anything else in the $PATH and voil\u00e1. Well, it wasn't that simple, but the other requirements were still met by this library. Despite requiring a Python installation in the system which your program supports, which I found later when I was trying to run a Python 3.12 application in a system that only had Python 3.8 installed, all dependencies are bundled and shipped together. pyinstaller docs are great and will let you be self-sufficient for your use cases - no need to explain what I need as it was really simple. One note is that pyinstaller 's will build a file for the platform you're building from (so, x86_64 linux will only work in x86_64 linux, arm will work in arm, etc) but this is a limitation I can live with. On my development machine I can generate and run the ARM builds, and the project's pipelines take care of building and publishing the binaries for the systems where they will run. The main advantage in my perspective is that everything gets bundled into a single executable and you don't need to care about environments, dependencies and what-not. Perhaps this will lead to a bigger file size, but it's a trade-off I'm willing to take for the sake of simplicity during deployments.","title":"Pyinstaller for Python apps bundling"},{"location":"notes/5/","text":"Getting Started in CyberSec in Portugal # For the past week, I saw a few messages in one Portuguese developer community from kids coming out of college wanting to get into Cyber Security and didn't know where to start. Although there are dozens (hundreds? thousands?) resources out there about this, I truly feel they overcomplicate things and this is the right platform to make a list on some plans those folks can consider to get started. 1. Foundational Skills # I personally advise everyone to know some things about programming. Sooner or later, it will be useful for you to either write your own exploits, tools or scripts, or to simply read code and be comfortable doing so. Python is commonly used in cyber security, as well as in other areas of IT, so I guess that's a good place to start. 2. Certifications Route # There is a parallel industry for certifications in cyber security which might overwhelm those who are starting. If you wish to get started into pentesting, the eJPT from eLearnSecurity is a great, cheap starting point. Another flagship certification for getting you a job is OSCP but despite being marketed as an entry-level certification, it might not be adequate for newcomers into IT for two reasons: pricing ]nd knowledge. Build your foundation, you can take on this exam later. TCM Security has 2 certifications that can be a good deal for you: PNPT and PJPT . The internet says knowledge-wise they are superior to eJPT and others, including the OSCP, but they aren't as established in the industry as they are newer. If you have the budget they might be an option for you. 3. Self-practitioner Route # For some/most Portuguese folks, certifications are way out of budget (unless they are getting company-sponsored but I assume this will not be case as we're referring to fresh-graduates). The alternative is to get your hands in the dirt and put in the time. Start working out through TryHackMe which is great for newbies as it holds your hand and has content for newbies and specific topics. HackTheBox is also a great platform and it can be your next endeavour when you want a more challenging exercise. The next things you can do in this approach include a bunch of self-taught and self-motivated experiences. You can work your way through bug bounties, CTFs and even do security research of applications/services/libraries you use and like. Who knows what you will encounter. The cherry on top is to find one thing you like and have a vulnerability disclosure program you can securely report your findings, if any, and (maybe) get rewarded for it. 4. Writing # At this point, you should really start writing things out in the form of blog posts. Even if you haven't found a major 0-day on a big library or you aren't rich through bug bounties, I find people that write their thought process and talk about their failures way more valuable than those who don't. It's simple: you're sharing knowledge and explaining your ideas and ways of looking at things. This will be a great thing for recruiters as they can judge your capabilities through your work. I personally think people really like to flourish the experiences they have on research and bug bounties, so let me stress out how hard this is and that it is totally okay to not find anything at all . The process and learning you get out of it is 100% what matters in the end. Perhaps you didn't find anything because you didn't know about a system or protocol, but know you can study more. Then after you have some more insights you'll conclude there are no security vulnerabilities there - but at least you learnt a new protocol/service/framework and in the next project of yours you'll be better prepared. Aim to learn about those systems, and not to make money. 5. Networking # I can't stress this enough. Cyber security is a niche market within the IT field. Get to know the people in the field, attend meetups, conferences, and even submit talks to those. Introduce yourself, don't be afraid to ask questions and learn from them. From experience, everybody is super welcoming and willing to help you out. Keep in mind is mostly through networking that the greatest opportunities will come to you. Although I'm no career guru to give you any advice on anything at all, I believe these five simple, yet hardworking steps, will really get you an advantage to get you started in the field. Good luck!","title":"Getting Started in CyberSec in Portugal"},{"location":"notes/5/#getting-started-in-cybersec-in-portugal","text":"For the past week, I saw a few messages in one Portuguese developer community from kids coming out of college wanting to get into Cyber Security and didn't know where to start. Although there are dozens (hundreds? thousands?) resources out there about this, I truly feel they overcomplicate things and this is the right platform to make a list on some plans those folks can consider to get started.","title":"Getting Started in CyberSec in Portugal"},{"location":"notes/5/#1-foundational-skills","text":"I personally advise everyone to know some things about programming. Sooner or later, it will be useful for you to either write your own exploits, tools or scripts, or to simply read code and be comfortable doing so. Python is commonly used in cyber security, as well as in other areas of IT, so I guess that's a good place to start.","title":"1. Foundational Skills"},{"location":"notes/5/#2-certifications-route","text":"There is a parallel industry for certifications in cyber security which might overwhelm those who are starting. If you wish to get started into pentesting, the eJPT from eLearnSecurity is a great, cheap starting point. Another flagship certification for getting you a job is OSCP but despite being marketed as an entry-level certification, it might not be adequate for newcomers into IT for two reasons: pricing ]nd knowledge. Build your foundation, you can take on this exam later. TCM Security has 2 certifications that can be a good deal for you: PNPT and PJPT . The internet says knowledge-wise they are superior to eJPT and others, including the OSCP, but they aren't as established in the industry as they are newer. If you have the budget they might be an option for you.","title":"2. Certifications Route"},{"location":"notes/5/#3-self-practitioner-route","text":"For some/most Portuguese folks, certifications are way out of budget (unless they are getting company-sponsored but I assume this will not be case as we're referring to fresh-graduates). The alternative is to get your hands in the dirt and put in the time. Start working out through TryHackMe which is great for newbies as it holds your hand and has content for newbies and specific topics. HackTheBox is also a great platform and it can be your next endeavour when you want a more challenging exercise. The next things you can do in this approach include a bunch of self-taught and self-motivated experiences. You can work your way through bug bounties, CTFs and even do security research of applications/services/libraries you use and like. Who knows what you will encounter. The cherry on top is to find one thing you like and have a vulnerability disclosure program you can securely report your findings, if any, and (maybe) get rewarded for it.","title":"3. Self-practitioner Route"},{"location":"notes/5/#4-writing","text":"At this point, you should really start writing things out in the form of blog posts. Even if you haven't found a major 0-day on a big library or you aren't rich through bug bounties, I find people that write their thought process and talk about their failures way more valuable than those who don't. It's simple: you're sharing knowledge and explaining your ideas and ways of looking at things. This will be a great thing for recruiters as they can judge your capabilities through your work. I personally think people really like to flourish the experiences they have on research and bug bounties, so let me stress out how hard this is and that it is totally okay to not find anything at all . The process and learning you get out of it is 100% what matters in the end. Perhaps you didn't find anything because you didn't know about a system or protocol, but know you can study more. Then after you have some more insights you'll conclude there are no security vulnerabilities there - but at least you learnt a new protocol/service/framework and in the next project of yours you'll be better prepared. Aim to learn about those systems, and not to make money.","title":"4. Writing"},{"location":"notes/5/#5-networking","text":"I can't stress this enough. Cyber security is a niche market within the IT field. Get to know the people in the field, attend meetups, conferences, and even submit talks to those. Introduce yourself, don't be afraid to ask questions and learn from them. From experience, everybody is super welcoming and willing to help you out. Keep in mind is mostly through networking that the greatest opportunities will come to you. Although I'm no career guru to give you any advice on anything at all, I believe these five simple, yet hardworking steps, will really get you an advantage to get you started in the field. Good luck!","title":"5. Networking"},{"location":"notes/6/","text":"Change Freezes Done Right # In many mature companies, the product development process is often halted for a period to avoid changes that could impact product stability and, consequently, the company's business during critical times. Think of changes to the Steam store on Christmas or adjustments to the Amazon website on Black Friday. You can find a good article on this practice here . Note: Yes, I am aware that a change freeze doesn't necessarily mean stop working. On the contrary, many people still do a bunch of work during these periods; they just can't deploy that work into production. In most cases, teams deploy things into development environments and then wait for the end of the freeze to deploy everything. Despite being a good practice, I am slightly against it in the models companies use, and this note is to state what I think it should be instead under normal circumstances (there are obvious exceptions at times that might justify freezing everything): Duration : The period should be short, ideally a week or two. Anything longer is a sign of a broken process, a flawed code base, or inadequate tools. People can ignore it, sure, but the root cause of instability still exists and will impact the product regardless of the change freeze. There is an obvious exception for when the critical period extends longer, but you get the idea. In the example of Steam given above, perhaps the period is justified to be longer than 2 weeks during Christmas because it's not only on Christmas Eve people buy games - but more so during the whole holiday season. Allowing Changes : Despite typically no one being allowed to make changes, some teams' work does not really impact the product. As such, this concept should be reviewed based on two main factors: The nature of the team's work - DevOps teams can impact the CI/CD but rarely the product itself (depends on the work, obviously; they aren't going to do some database upgrades in this period). But I don't understand the problem with continuing to work on platform automation, for instance. Again, with the right balance, things should still be possible. The history of the team or tribe's responsibility causing incidents in the past . This last point is critical and is a game-changer for most. Teams that cause instability over time should be incentivized to fix their processes, tools, and/or products. Maybe this will make them rethink their technical debt, enabling them to work during these periods. Of course, this might not suffice, but at least it doesn't impact other teams' work if we take into account the other suggestions. Some may say this is a form of finger-pointing, but I find it reasonable enough to be worth it. It's a form of gamification and a way to make people think about the consequences of their processes, tools, and products (including the code base). I'm not judging those who do their best to keep things running and are hands-tied to actually fix them. Companies nowadays often throw money at problems when sometimes they should just solve the problems instead. I'm a fan of KISS, so I work wholeheartedly to make the things I work on simple and easy to work with. I'm happy when a colleague mentions that a given process, tool, or whatever is well-documented, allowing them to understand where they should be headed and get their work done. It's really painful to maintain legacy things, and I truthfully wonder if it doesn't cost more money maintaining the legacy instead of fixing things around. Perhaps the business can't stop entirely to make it work, but maybe improving things gradually is the way to go. Neither seems to happen, and then high-level management came up with change freezes, which stops everyone from continuing with the agile lifecycle.","title":"Change Freezes Done Right"},{"location":"notes/6/#change-freezes-done-right","text":"In many mature companies, the product development process is often halted for a period to avoid changes that could impact product stability and, consequently, the company's business during critical times. Think of changes to the Steam store on Christmas or adjustments to the Amazon website on Black Friday. You can find a good article on this practice here . Note: Yes, I am aware that a change freeze doesn't necessarily mean stop working. On the contrary, many people still do a bunch of work during these periods; they just can't deploy that work into production. In most cases, teams deploy things into development environments and then wait for the end of the freeze to deploy everything. Despite being a good practice, I am slightly against it in the models companies use, and this note is to state what I think it should be instead under normal circumstances (there are obvious exceptions at times that might justify freezing everything): Duration : The period should be short, ideally a week or two. Anything longer is a sign of a broken process, a flawed code base, or inadequate tools. People can ignore it, sure, but the root cause of instability still exists and will impact the product regardless of the change freeze. There is an obvious exception for when the critical period extends longer, but you get the idea. In the example of Steam given above, perhaps the period is justified to be longer than 2 weeks during Christmas because it's not only on Christmas Eve people buy games - but more so during the whole holiday season. Allowing Changes : Despite typically no one being allowed to make changes, some teams' work does not really impact the product. As such, this concept should be reviewed based on two main factors: The nature of the team's work - DevOps teams can impact the CI/CD but rarely the product itself (depends on the work, obviously; they aren't going to do some database upgrades in this period). But I don't understand the problem with continuing to work on platform automation, for instance. Again, with the right balance, things should still be possible. The history of the team or tribe's responsibility causing incidents in the past . This last point is critical and is a game-changer for most. Teams that cause instability over time should be incentivized to fix their processes, tools, and/or products. Maybe this will make them rethink their technical debt, enabling them to work during these periods. Of course, this might not suffice, but at least it doesn't impact other teams' work if we take into account the other suggestions. Some may say this is a form of finger-pointing, but I find it reasonable enough to be worth it. It's a form of gamification and a way to make people think about the consequences of their processes, tools, and products (including the code base). I'm not judging those who do their best to keep things running and are hands-tied to actually fix them. Companies nowadays often throw money at problems when sometimes they should just solve the problems instead. I'm a fan of KISS, so I work wholeheartedly to make the things I work on simple and easy to work with. I'm happy when a colleague mentions that a given process, tool, or whatever is well-documented, allowing them to understand where they should be headed and get their work done. It's really painful to maintain legacy things, and I truthfully wonder if it doesn't cost more money maintaining the legacy instead of fixing things around. Perhaps the business can't stop entirely to make it work, but maybe improving things gradually is the way to go. Neither seems to happen, and then high-level management came up with change freezes, which stops everyone from continuing with the agile lifecycle.","title":"Change Freezes Done Right"},{"location":"notes/7/","text":"Django and Reverse M2M Relationship Filter # While working on a project today, I learnt it's possible to execute reverse many-to-many relationship filters directly in Django. from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Tag(models.Model): name = models.CharField(max_length=100) author = models.ForeignKey('Author', on_delete=models.CASCADE) class Post(models.Model): title = models.CharField(max_length=100) tags = models.ManyToManyField(Tag) In a scenario you need to find all posts with tags created by user, you can do the following: post = Post.objects.filter(tags__author__name=\"author_name\") Much better and cleaner than chaining different querysets, right? SQL-wise, it also seems more efficient. The only thing that is missing there is the prefetch_related which again can be added to the queryset to avoid the N+1 problem: post = Post.objects.filter(tags__author__name=\"tag_name\").prefetch_related('tags_set__author') The addition there is the _set which is the default name for the reverse relation lookups.","title":"Django and Reverse M2M Relationship Filter"},{"location":"notes/7/#django-and-reverse-m2m-relationship-filter","text":"While working on a project today, I learnt it's possible to execute reverse many-to-many relationship filters directly in Django. from django.db import models class Author(models.Model): name = models.CharField(max_length=100) class Tag(models.Model): name = models.CharField(max_length=100) author = models.ForeignKey('Author', on_delete=models.CASCADE) class Post(models.Model): title = models.CharField(max_length=100) tags = models.ManyToManyField(Tag) In a scenario you need to find all posts with tags created by user, you can do the following: post = Post.objects.filter(tags__author__name=\"author_name\") Much better and cleaner than chaining different querysets, right? SQL-wise, it also seems more efficient. The only thing that is missing there is the prefetch_related which again can be added to the queryset to avoid the N+1 problem: post = Post.objects.filter(tags__author__name=\"tag_name\").prefetch_related('tags_set__author') The addition there is the _set which is the default name for the reverse relation lookups.","title":"Django and Reverse M2M Relationship Filter"},{"location":"notes/8/","text":"Use bulks, please # In a recent change we were performing, we needed to update our database records in a magnitude of hundreds of thousands. Did so many things before, twisted and squeeze the hell out of Django's ORM (might not be the best in the industry, but it is what it is). Our deployment pipelines were failing because the migrations would either time out or just take forever to finish, causing locks and other bottlenecks in the applications. It got to a point I just grew out of it and went with bulk_create() and bulk_update where needed. update_or_create() performs two queries under the hood, this runs one for all creates and all updates. It assumes no object or all objects exist, respectively, and there are some options to find the middle ground there too, like ignore_conflicts in case of bulk_create . This also assumes good database architecture and defaults, meaning unique constraints are properly followed and implemented in a way that will not interfere when running the bulks. Performance-wise, it went from 5 minutes to 5 seconds locally, from a never-ending change in production to 20 minutes. Add batch_size if you're concerned and want to somehow throttle the number of objects that are created/updated. Refs: https://docs.djangoproject.com/en/5.0/ref/models/querysets/#bulk-create","title":"Use bulks, please"},{"location":"notes/8/#use-bulks-please","text":"In a recent change we were performing, we needed to update our database records in a magnitude of hundreds of thousands. Did so many things before, twisted and squeeze the hell out of Django's ORM (might not be the best in the industry, but it is what it is). Our deployment pipelines were failing because the migrations would either time out or just take forever to finish, causing locks and other bottlenecks in the applications. It got to a point I just grew out of it and went with bulk_create() and bulk_update where needed. update_or_create() performs two queries under the hood, this runs one for all creates and all updates. It assumes no object or all objects exist, respectively, and there are some options to find the middle ground there too, like ignore_conflicts in case of bulk_create . This also assumes good database architecture and defaults, meaning unique constraints are properly followed and implemented in a way that will not interfere when running the bulks. Performance-wise, it went from 5 minutes to 5 seconds locally, from a never-ending change in production to 20 minutes. Add batch_size if you're concerned and want to somehow throttle the number of objects that are created/updated. Refs: https://docs.djangoproject.com/en/5.0/ref/models/querysets/#bulk-create","title":"Use bulks, please"},{"location":"notes/9/","text":"TIL: QuerySets Over Multiple Tables May Not Return Unique Values # Today I learned that when you query multiple tables in Django, you may not get unique values. This is specially true when the query is somehow crossing over multiple tables (filtering over relationships, for instance) and then doing the joining back in Python. We have a unit test that started failing after some work and the root cause was a duplicated object, which had me scratching me head for a while. Then it clicked that this could be the use case of distinct() and when I was ready the documentation, I was right - I had see so many distinct() calls before but never thought this was the reason. Not sure what I thought really, but had me laughing at myself after figuring out the problem. While also reviewing things, I learned that we had a mistake in our unit tests that had to be fixed. References: https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.distinct","title":"TIL: QuerySets Over Multiple Tables May Not Return Unique Values"},{"location":"notes/9/#til-querysets-over-multiple-tables-may-not-return-unique-values","text":"Today I learned that when you query multiple tables in Django, you may not get unique values. This is specially true when the query is somehow crossing over multiple tables (filtering over relationships, for instance) and then doing the joining back in Python. We have a unit test that started failing after some work and the root cause was a duplicated object, which had me scratching me head for a while. Then it clicked that this could be the use case of distinct() and when I was ready the documentation, I was right - I had see so many distinct() calls before but never thought this was the reason. Not sure what I thought really, but had me laughing at myself after figuring out the problem. While also reviewing things, I learned that we had a mistake in our unit tests that had to be fixed. References: https://docs.djangoproject.com/en/dev/ref/models/querysets/#django.db.models.query.QuerySet.distinct","title":"TIL: QuerySets Over Multiple Tables May Not Return Unique Values"}]}